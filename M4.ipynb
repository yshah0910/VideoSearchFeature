{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buqhHUl0cbAH",
        "outputId": "8cba5661-304b-4e25-92cd-c35444fdd813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h17dnrhe5OH",
        "outputId": "22997c83-d174-4f1a-98a6-406b95b359fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q -U tensorflow-hub tensorflow-text tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru_CizrufIlD",
        "outputId": "f4411d56-1fd0-43c0-9ebc-71911af291a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AafejiROe18F"
      },
      "source": [
        "## Projection head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAGx7Ma3evvY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load video embeddings\n",
        "#def load_video_embeddings(video_num):\n",
        " #   return np.load(f\"/content/drive/MyDrive/CS482-Milestone4/combined_embeddings_folder/video{video_num}_combined_embedding.npy\") #video1_combined_embedding\n",
        "\n",
        "def load_video_embeddings(video_num):\n",
        "    # Load video embeddings\n",
        "    #video_embeddings = np.load(f'/content/drive/MyDrive/CS482-Milestone4/combined_embeddings_folder/video{video_num}_combined_embedding.npy', allow_pickle=True)\n",
        "    video_embeddings = np.load(f'/content/drive/MyDrive/CS482-Milestone4/TextPembed/video{video_num}.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "    return video_embeddings\n",
        "\n",
        "\n",
        "# Load text embeddings\n",
        "def load_text_embeddings(video_num):\n",
        "    #return np.load(f'/content/drive/MyDrive/CS482-Milestone4/TextPembed/video{video_num}.npy', allow_pickle=True)\n",
        "    #text_embeddings = np.load(f'/content/drive/MyDrive/CS482-Milestone4/combined_embeddings_folder/video{video_num}_combined_embedding.npy', allow_pickle=True)\n",
        "    text_embeddings = np.load(f'/content/drive/MyDrive/CS482-Milestone4/TextPembed/video{video_num}.npy', allow_pickle=True)\n",
        "\n",
        "    return text_embeddings\n",
        "\n",
        "# Reshape the input data to match the expected shape for Dense layers\n",
        "def reshape_input_data(data):\n",
        "    # Assuming data is one-dimensional, reshape it to (batch_size, features)\n",
        "    reshaped_data = data.reshape(data.shape[0], -1)  # Replace -1 with the appropriate number of features\n",
        "\n",
        "    return reshaped_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EByZJ4Btev6r"
      },
      "outputs": [],
      "source": [
        "# Projection head function\n",
        "def projection_head(video_embeddings, text_embeddings, projection_dim):\n",
        "    # Implement your projection head logic using Dense layers or other desired methods\n",
        "    # Transform video and text embeddings into the joint embedding space with the given dimensionality\n",
        "    video_input = keras.layers.Input(shape=video_embeddings.shape[1:])\n",
        "    text_input = keras.layers.Input(shape=text_embeddings.shape[1:])\n",
        "\n",
        "    # Projection layers for video embeddings\n",
        "    video_projection = keras.layers.Dense(projection_dim, activation='relu')(video_input)\n",
        "\n",
        "    # Projection layers for text embeddings\n",
        "    text_projection = keras.layers.Dense(projection_dim, activation='relu')(text_input)\n",
        "\n",
        "    # Return the transformed video and text embeddings\n",
        "    return video_projection, text_projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9B-GZjxiti7"
      },
      "outputs": [],
      "source": [
        "def projection_head(video_embeddings, text_embeddings, projection_dim):\n",
        "    # Implement projection head logic using Dense layers or desired methods\n",
        "    # Transform video and text embeddings into the joint embedding space with the given dimensionality\n",
        "\n",
        "    # Assuming you're using Dense layers for projection\n",
        "    video_input = keras.layers.Input(shape=video_embeddings.shape[1:])\n",
        "    text_input = keras.layers.Input(shape=text_embeddings.shape[1:])\n",
        "\n",
        "    # Projection layers for video embeddings\n",
        "    projected_video = keras.layers.Dense(projection_dim, activation='relu')(video_input)\n",
        "\n",
        "    # Projection layers for text embeddings\n",
        "    projected_text = keras.layers.Dense(projection_dim, activation='relu')(text_input)\n",
        "\n",
        "    # Reshape projected_video and projected_text to 2D\n",
        "    projected_video = keras.layers.Flatten()(projected_video)\n",
        "    projected_text = keras.layers.Flatten()(projected_text)\n",
        "\n",
        "    return projected_video, projected_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpG0zKezfXmm"
      },
      "source": [
        "## Contrastive loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HslRXLIUev9O"
      },
      "outputs": [],
      "source": [
        "# Contrastive loss function\n",
        "def contrastive_loss(y_true, y_pred, margin=1):\n",
        "    # Calculate contrastive loss\n",
        "    square_pred = tf.square(y_pred)\n",
        "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
        "    return tf.reduce_mean((1 - y_true) * square_pred + y_true * margin_square)\n",
        "\n",
        "# Prepare data for contrastive learning\n",
        "def prepare_contrastive_data(video_embeddings, text_embeddings):\n",
        "    # Implement logic to create pairs of video-text embeddings and their labels (similar or dissimilar)\n",
        "    # For simplicity, create pairs randomly labeled as similar or dissimilar\n",
        "    num_samples = min(video_embeddings.shape[0], text_embeddings.shape[0])\n",
        "    labels = np.random.choice([0, 1], size=(num_samples,))\n",
        "    return video_embeddings[:num_samples], text_embeddings[:num_samples], labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8m50clNfcex"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "8nfxLRZirDkL",
        "outputId": "b0b21bf8-9c35-4b29-afd4-8ae777bcf0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video Embeddings Shape: (25486, 300)\n",
            "Text Embeddings Shape: (25486, 300)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-f2e042735dc3>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Reshape X_train_text and X_val_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mX_train_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mX_val_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_video\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Repeat to match X_val_video size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 6116400 into shape (256)"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "#video_embeddings_folder = \"/content/drive/MyDrive/CS482-Milestone4/combined_embeddings_folder\"\n",
        "video_embeddings_folder = \"/content/drive/MyDrive/CS482-Milestone4/TextPembed\"\n",
        "\n",
        "# Load video embeddings\n",
        "video_embeddings = []\n",
        "for video_num in range(1, 51):\n",
        "    #video_embedding = np.load(f'{video_embeddings_folder}/video{video_num}_combined_embedding.npy', allow_pickle=True)\n",
        "    video_embedding = np.load(f'{video_embeddings_folder}/video{video_num}.npy', allow_pickle=True)\n",
        "\n",
        "    video_embeddings.append(video_embedding)\n",
        "text_embeddings_folder = \"/content/drive/MyDrive/CS482-Milestone4/TextPembed\"\n",
        "#text_embeddings_folder = \"/content/drive/MyDrive/CS482-Milestone4/combined_embeddings_folder\"\n",
        "\n",
        "\n",
        "# Load text embeddings\n",
        "text_embeddings = []\n",
        "for video_num in range(1, 51):\n",
        "    text_embedding = np.load(f'{text_embeddings_folder}/video{video_num}.npy', allow_pickle=True)\n",
        "    #text_embedding = np.load(f'{video_embeddings_folder}/video{video_num}_combined_embedding.npy', allow_pickle=True)\n",
        "\n",
        "    text_embeddings.append(text_embedding)\n",
        "\n",
        "# Combine all video embeddings\n",
        "video_embeddings = np.concatenate(video_embeddings, axis=0)  # Use axis=0 to concatenate along the first axis\n",
        "\n",
        "# Combine all text embeddings\n",
        "text_embeddings = np.concatenate(text_embeddings, axis=0)  # Use axis=0 to concatenate along the first axis\n",
        "print(\"Video Embeddings Shape:\", video_embeddings.shape)\n",
        "print(\"Text Embeddings Shape:\", text_embeddings.shape)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train_video, X_val_video, X_train_text, X_val_text = train_test_split(video_embeddings, text_embeddings, test_size=0.2)\n",
        "\n",
        "num_features = 256\n",
        "\n",
        "# Reshape X_train_text and X_val_text\n",
        "X_train_text = X_train_text.reshape((-1, num_features))\n",
        "X_val_text = np.repeat(X_val_text, len(X_val_video) // len(X_val_text), axis=0)  # Repeat to match X_val_video size\n",
        "\n",
        "# Reshape X_val_video and X_train_video\n",
        "X_val_video = X_val_video.reshape((-1, num_features))\n",
        "X_train_video = X_train_video.reshape((-1, num_features))\n",
        "\n",
        "# Create dummy labels for training and validation\n",
        "y_train_dummy = np.random.rand(X_train_video.shape[0])\n",
        "y_val_dummy = np.random.rand(X_val_video.shape[0])\n",
        "\n",
        "\n",
        "#Create the projection head model\n",
        "projection_input_video = keras.layers.Input(shape=X_train_video[0].shape)\n",
        "projection_input_text = keras.layers.Input(shape=X_train_text[0].shape)\n",
        "\n",
        "projected_video = keras.layers.Dense(128, activation='relu')(projection_input_video)\n",
        "projected_text = keras.layers.Dense(128, activation='relu')(projection_input_text)\n",
        "\n",
        "projection_model = keras.Model(inputs=[projection_input_video, projection_input_text], outputs=[projected_video, projected_text])\n",
        "\n",
        "# Create the contrastive learning model\n",
        "contrastive_input_video = keras.layers.Input(shape=(128,))\n",
        "contrastive_input_text = keras.layers.Input(shape=(128,))\n",
        "\n",
        "contrastive_output = keras.layers.Concatenate()([contrastive_input_video, contrastive_input_text])\n",
        "contrastive_output = keras.layers.Dense(1, activation='linear')(contrastive_output)\n",
        "\n",
        "contrastive_model = keras.Model(inputs=[contrastive_input_video, contrastive_input_text], outputs=contrastive_output)\n",
        "\n",
        "# Compile the contrastive learning model\n",
        "contrastive_model.compile(optimizer='adam', loss=contrastive_loss)\n",
        "\n",
        "# Create early stopping and learning rate scheduler callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "learning_rate_scheduler = LearningRateScheduler(lambda epoch: 0.001 * 0.5 ** (epoch // 10))\n",
        "\n",
        "history = contrastive_model.fit(\n",
        "    [X_train_video, X_train_text],\n",
        "    y_train_dummy,\n",
        "    epochs=50,\n",
        "    validation_data=([X_val_video, X_val_text], y_val_dummy),\n",
        "    callbacks=[early_stopping, learning_rate_scheduler]\n",
        ")\n",
        "\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
